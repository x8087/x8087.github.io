title: man Wget
date: 2015-11-27 14:20:40
categories: 技术
tags: [Wget,unix,linux]
---

### Wget命令
wget [option]... [URL]...

wget命令用来从指定的URL下载文件。
wget非常稳定，它在带宽很窄的情况下和不稳定网络中有很强的适应性，
如果是由于网络的原因下载失败，wget会不断的尝试，直到整个文件下载完毕。
如果是服务器打断下载过程，它会再次联到服务器上从停止的地方继续下载。
这对从那些限定了链接时间的服务器上下载大文件非常有用。

Wget能根据HTML,XHTML和CSS页面，创建远程网站的本地版本，完整重建原始站点的目录结构。这个有时候称为“递归下载”。

<!-- more -->
####  基本启动选项 ####
##### -V #####
###### --version ######

显示Wget版本
##### -h #####
###### --help ######

打印所有Wget命令行选项帮助信息描述
##### -b #####

###### --background ######

启动后立即转到后台。如果没有经由-o选项指定输出文件，输出重新调配到wget-log。

##### -e command #####

--execute command
把命令作为.wgetrc的一部分执行。这样的命令将在.wgetrc的命令后执行，因而优先于它们。如果需要指定更多wgetrc命令，使用多个-e实例。

#### 记录和输入文件选项 ####

##### -o logfile #####

--output-file=logfile
记录所有信息到logfile。信息通常向标准错误报告。

##### -a logfile #####

--append-output=logfile
添加到logfile。和-o一样，只不过它附加到logfile替代重写旧的记录文件。如果logfile不存在，创建新文件。

##### -d #####

--debug
打开调试输出，意味许多对Wget开发者重要的消息如果没有正确地工作。你的系统管理员可能选择编译Wget没有调试支持，使-d不能工作。请注意：带调试支持编译总是安全的——Wget带调试支持编译不打印任何调试信息除非使用-d请求。

##### -q #####

--quiet
关闭Wget输出。

##### -v #####

--verbose
打开详细输出，带所有可获取的数据。默认输出是详细。

##### -nv #####

--no-verbose
关闭详细，不变成完全沉默（使用-q）。意味错误信息和基本消息仍然打印。

##### --report-speed=type #####

作为type的输出带宽。只接受值bits。

##### -i file #####

--input-file=file
从本地或者外部file读取URL。如果-指定作为file，从标准输入读取URL。（使用./-从字面上命名为-的文件读取。）

如果使用这个功能，没有URL需要呈现在命令行。如果URL在命令行和输入文件上两个都有，命令行将首先读取。如果--force-html未指定，则文件应该由一系列URL组成，每行一个。

然而，如果你指定--force-html，文档将作为html看待。这种情况你可能存在相对链接的问题，你可以通过添加“<base href="url">”到文档或者在命令行指定--base=url指定来解决问题。

如果文件是外部的，如果文档的Content-Type匹配text/html将自动地作为html对待。此外，文件路径将默认作为基础href如果没有指定。

##### -F #####

--force-html
当输入从文件读取时，强制它当作html文件对待。这允许你从本地磁盘存在的html文件检索相对链接，添加“<base href="url">”到HTML，或者使用--base命令行选项。

##### -B URL #####

--base=URL
解决相对链接使用URL作为参考点，当从HTML文件读取链接通过-i/--input-file选项时（与--force-html，或者当输入文件是从服务器远程取得描述成HTML）。这相当于在HTML输入文件里存在“BASE”标签，用URL作为“href”属性的值。

例如，如果指定http://foo/bar/a.html为URL，Wget从输入文件读取../baz/b.html，将解析成http://foo/baz/b.html。

##### --config=FILE #####

指定你希望使用的启动文件路径。

#### 下载选项 ####

##### --bind-address=ADDRESS #####

当制作客户端TCP/IP连接时，绑定到在本地机器上的ADDRESS。ADDRESS可以指定为主机名或者IP地址。如果你的机器被限制多个IP这个选项非常有用。

##### -t number #####

--tries=number
设置尝试次数为number。指定0或者inf为无限尝试。默认是尝试20次，除非遇到致命错误像“connection refused”或者“not found”（404）则不尝试。

##### -O file #####

--output-document=file
文档将不会写到相应的文件，但是所有将串联一起并写到file。如果-作为file使用，文档将打印到标准输出，停用链接转换。（使用./-打印字面上命名为-的文件）
使用-O不是简单意味“使用file名字替代URL中的一个；”当然，它类似于shell重定向：wget -O file http://foo是打算工作像wget -O - http://foo>file；file将立即缩短，所有下载内容将被写进那里。

为些，-N（时间戳检查）不支持与-O组合：因为file总是最近创建，它将总是有最新的时间戳。如果使用这个组合会发布警告。

类似的，使用-r或者-p与-O可能不会如你期望的工作：Wget不会只下载第一个文件归档然后下载其他他们的普通名字：所有下载内容将放入file。这个在1.11版本信用，但是在1.11.2版本恢复（带警告），在某些情况下这个行为实际上有一些用处。

注意这个与-k组合只允许在下载单个文档时，在这种情况它将只转换所有相对URI为外部的；-k使对多个URI当他们所有被下载到单个文件时没有作用；-k仅当输出是一个正则文件能被用。

##### -nc #####

--no-clobber
如果文件被下载超过一次到相同的目录，Wget的行为取决于几个选项，包含-nc。在某些情况，本地文件将被重写，或者覆盖，根据重复下载。另一些情况它将被保存。

当运行Wget没有-N，-nc，-r或者-p，下载相同文件到相同目录将导致原始file副本被保存且第二个副本被命名为file.1。如果文件被再下载，第三个副本将被命名为file.2，等等。（这也是用-nd的行为，甚至在-r或者-p作用下。）当-nc指定，这个行为被抑止，且Wget将拒绝下载新的file副本。因此，“no-clobber”在这个模式实际上是误称——它没有重写而是防止（如数字后缀就已经是防止重写），而宁可说是多个版本保存来防止。

当运行Wget带-r或者-p，但是没有-N，-nd或者-nc，重新下载文件将导致新的副本简单地覆盖旧的。添加-nc将防止这种行为，反而引起原始版本被保存，而其他服务器上新的副本被忽略。

当运行Wget带-N，带或者没有-r或者-p，决定是否下载新的文件副本取决于本地与远程文件的时间戳和大小。-nc可能不被指定在-N的同时。

注意当-nc指定，文件带后缀.html或者.htm将被从本地磁盘加载并解析如他们从网络检索到。

##### --backups=backups #####

写文件前，通过添加一个.1后缀到文件名备份存在的文件。这样的备份文件循环为.2，.3等等，一直到backups（并且丢失那边）。

##### -c #####

--continue


继续获取部分下载文件。当你需要完成上个Wget实例，或者另一个程序开始的下载时是有用的。例如：
wget -c ftp://sunsite.doc.ic.ac.uk/ls-lR.Z
如果当前目录有文件命名为ls-lR.z，Wget将假定它是远程文件的第一部分，询问服务器从等于本地文件长度偏移数继续恢复。

注意如果你只想当前Wget调用重新尝试下载连接中途丢失的文件不需要指定选项。这个是默认行为。-c只影响下载恢复开始在Wget调用之前的，本地文件仍然闲置的。

没有-c，先前的例子将只下载远程文件到ls-lR.z.1，保留截断的ls-lR.z文件。

Wget1.7开始，如果你在非空文件上用-c，它的结果是服务器不支持继续下载，Wget将拒绝从暂时开始下载，那个将有效地破坏存在内容。如果你真的需要从暂时开始下载，移除文件。

也是Wget1.7开始，如果你在一个文件大小和服务器上大小相等的文件上用-c，Wget将拒绝下载并打印解释的信息。同样的当服务器上的文件比本地的小时（大概因为它自从你的上次下载尝试后在服务器上被改变）——因为“继续”没有意义，没有下载发生。

另一方面，在使用-c时，任何文件服务器上比本地大会被认为是不安全下载并下载合适的（length(remote)-length(local)）字节附加到本地文件的尾端。这个行为在某些情况能令人满意的——例如，你能用wget -c下载恰好添加到数据集合或者记录文件的新部分。

然而，如果文件在服务器上变大是因为改变了，与附加正好相对，你将以一个混乱的文件结束。Wget没有办法核实核实本地文件是真的远程文件的有效前缀。你需要特别小心当-c与-r结合使用时，因为每个文件将被认为是“不安全下载”候选。

另一个会得到混乱的文件的例子是你尝试用-c，如果有一个差劲的HTTP代理插入一段“transfer interrupted”字符串到本地文件。将来可能添加一个“rollback”选项处理这种情况。

注意-c只工作在支持“Range”头的FTP服务器和HTTP服务器。

##### --start-pos=OFFSET #####

在从零开始的位置OFFSET开始下载。偏移可能用字节表示，千字节带“k”后缀，兆字节带“m”后缀，等等。

--start-pos有更高的优先级超过--continue。当--start-pos和--continue两个指定时，wget发出一个警告并当作缺少--continue处理。

服务器必需支持继续下载，否则--start-pos不能超作用。详细参考-c。

##### --progress=type #####

选择你希望使用的进度指示器类型。合法的指示器是“dot”和“bar”。
默认使用“bar”指示器。画一个ASCII进度条图形（a.k.a "thermometer"(温度计)display）指示检索状态。如果输出不是TTY，默认使用“dot”条。

用--progress=dot切换到“dot”显示。在屏幕上打印点来跟踪检索，每个点表示确定数量的下载数据。

进度类型type也能带一到多个参数。参数基于选择的类型改变。type的参数传递添加到类型用冒号（:）分隔像这样：--progress=type:parameter1:parameter2。

当使用点检索，你可以设定指定类型的样式如dot:style。不同样式对一个点分配不同意义。默认样式每个点表示1K，一组10点且一行50个点。"binary"样式更加“可算（Computer）”——像定向——8
k点，16点串48点每行（384k行）。“mega”样式适合下载大文件——每点表示64k检索，8点一串，48点每行（每行含3M）。如果“mega”不够则使用“giga”样式——每点表示1M检索，8点一串，32点每行（每行含32M）。

用--progress=bar，有两个可能参数，force与noscroll。

当输出不是TTY，进度条总是回退到“点”，甚至调用wget时传入--progress=bar。这个行为能被覆盖强制“bar”输出使用“force”参数如--progress=bar::force。

默认，如果条样式进度条被下载的文件文件名超出分配它显示的最大长度，文件名从左向右滚动。在某些情况，像--progress=bar:force，可能不需要进度条滚动文件名。传入“noscroll”参数，wget能强制显示尽可能多地显示文件名而不滚动。

注意你能在.wgetrc使用“grogress”命令设置默认样式。这个设定能被命令行覆盖。例如，例如强制条输出不滚动，使用--progress=bar:force:noscroll。

##### --show-progress #####

强制wget在任何冗长里显示进度条。

默认，wget只在冗长模式显示进度条。然而你可能，需要wget在屏幕上结合其他模式显示进度条像--no-verbose或--quiet。这常常是一个要求属性当调用wget下载几个小/大文件时。在这种情况，wget能简单调用参数在屏幕上获取更加干净的输出。

当这个选项和--logfile选项一起使用时也强制进度条打印到标准错误（stderr）

##### -N #####

--timestamping
开启时间戳。

--no-use-server-timestamps
不设置本地文件服务器上的时间戳。

默认，当下载一个文件时，它的时间戳是设置为匹配远程文件。这允许在wget子序列调用上使用--timestamping。然而，它有时是有用的，根据本地文件时间戳判断它实际什么时候下载。由此目的，提供--no-use-server-timestamps选项

##### -S #####

###### --server-response ######

打印HTTP服务器发送的头部和FTP服务器发送的答复。

###### --spider ######

当使用这个选项调用时，wget表示为网络爬虫，意味着不下载页面，只是检查它们。例如，你能使用Wget检查你的书签：
wget --spider --force-html -i bookmarks.html
Wget的这个功能需要做更多接近实用的真实网络爬虫。

##### -T seconds #####

--timeout=seconds
设置网络超时为seconds秒。这等于指定--dns-timeout，--connect-timeout与--read-timeout所有同时。

当与网络交互时，Wget能检查超时并当花费太长时间时终止操作。这样防止像读取挂起与无限连接的异常。默认唯一可行的超时是900秒超时读取。设置超时为0完全禁止它。除非你知道你要做什么，最好不要改变默认超时设置。

所有超时相关选项接受小数值，和微秒值一样。例如，0.1秒是合法的（虽然不明智）超时选项。微秒超时对检查服务器响应时间或测试潜在的网络是有用的。

--dns-timeout=seconds
设置DNS检查超时为seconds秒。DNS检查没有在指定时间完成将失败。默认，DNS检查不会超时，除了系统库实现。

--connect-timeout=seconds
设置连接超时为seconds秒。TCP建立连接时间太长将终止。默认，没有连接超时，除了系统库实现。

--read-timeout=seconds
设置读取（写入）超时为seconds秒。这个超时“时间”参考空闲时间：如果，在下载的任何点，没有数据接收超过指定数目的秒数，读取失败且下载重新开始。这个选项不直接影响整个下载的持续。

当然，远程服务器可能选择早于这个选项所需时间终止连接。默认读取超时是900秒。

--limit-rate=amount
限制下载速度这amount字节每秒。总数可能用字节，千字节带k后缀，或者兆字节带m后缀表达。例如，--limit-rate=20k将限制检索速度为20KB/s。当你不需要Wget耗尽整个可获得的带宽时，无论什么原因，这是有用的。

这个选项允许使用小数，通常结合强大的后缀；例如，--limit-rate=2.5k是合法值。

注意Wget通过睡眠合适的数量的时间实现网络读取后比特定速率花费更少的时间。最终这个策略引起TCP传输减速到接近于指定速率。然而，它达到平衡可能花费一些时间，所以不要惊奇非常小的文件限制速率没有工作好。

-w seconds
--wait=seconds
在检索时等待指定秒数。推荐使用这个选项，它较少频繁地制造请求减轻服务器加载。代替秒，时间能使用“m”后缀指定分钟，“h”后缀指定小时，或者“d”指定天数。

如果网络或者目标主机停机，为其指定一个大的值是有用的，以便Wget能等待足够长合理地期待网络错误在重试之前被修正。这个功能指定的等待时间间隔受“--random-wait”影响，参见。

--waitretry=seconds
如果你不需要Wget在每个检索之间等待，但是只在失败下载重试之间，你能用这个选项。Wget将用线性补偿，在给定的文件首次失败后等待1秒，接着第二次失败后等待2秒，直到你指定的最大秒数。

默认，Wget将采用值10秒。

--random-wait
一些网站能通过记录来分析定义像Wget的检索程序，寻找在请求之间的时间统计上象征的相似之处。这选项引起请求之间的时间在0.5到1.5*wait秒之间变化，wait是使用--wait选项指定的，为了从这样的分析屏蔽Wget的存在。

--no-proxy
不使用代理，甚至合适的*_proxy环境变量定义了。

##### -Q quota #####

###### --quota=quota ######

指定自动检索的下载配额。值可以用字节（默认），千字节（后缀k），或者兆字节（后缀m）。

注意配额从不影响下载单个文件。所以如果你指定wget -Q10k ftp://wuarchive.wustl.edu/ls-lR.gz, 所有ls-lR.gz被下载。同样即使几个命令行上指定的URL。然而，配额是受遵守的当递归检索或者从输入文件。因此你可以安全地输入wget -Q2m -i sites---下载当配额超过时中止。

设置配额为0或者inf不限制下载配额。

###### --no-dns-cache ######

关闭DNS缓存检查。通常，Wget记得它从DNS查找的IP地址，所以它不必重复地联系DNS服务器同样的（通常很小的）主机设置它所检索的。这个缓存只存在在内存里；新的Wget运行将再次联系DNS。

然而，它已被报告，在某些情况下，它缓存主机名字是不可取的，即使一个简短运行程序持续时间，像Wget。使用此选项wget发布一个新的DNS查找（更准确地说，一个新的调用“gethostbyname”或“getaddrinfo”）每次做一个新的连接



